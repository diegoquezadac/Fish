{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch import nn\n",
    "from torchmetrics import Precision, Recall, F1Score, Accuracy\n",
    "from torch.utils.data import DataLoader,Dataset, WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/processed/train.csv\")\n",
    "df_val = pd.read_csv(\"../data/processed/val.csv\")\n",
    "df_test = pd.read_csv(\"../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178839, 2), (22355, 2), (22355, 2))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.x_train = X\n",
    "        self.y_train = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "    def get_sampler(self):\n",
    "        class_weights = [0.1, 0.9]\n",
    "        sample_weights = [0] * len(self)\n",
    "        for idx, (text, label) in enumerate(self):\n",
    "            sample_weights[idx] = class_weights[label]\n",
    "        sampler = WeightedRandomSampler(\n",
    "            sample_weights, num_samples=len(sample_weights), replacement=True\n",
    "        )\n",
    "        return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/dev/fish/.venv/lib/python3.10/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    df_train[\"text\"].values.tolist(), df_train[\"toxic\"].values.tolist(), tokenizer\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    df_val[\"text\"].values.tolist(), df_val[\"toxic\"].values.tolist(), tokenizer\n",
    ")\n",
    "test_dataset = CustomDataset(\n",
    "    df_test[\"text\"].values.tolist(), df_test[\"toxic\"].values.tolist(), tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(iterator, tokenizer):\n",
    "    for text,_ in iterator:\n",
    "        yield tokenizer(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = iter(train_dataset)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator, tokenizer), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196674"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ``collate_fn``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    text_pipeline = lambda x: vocab(tokenizer(str(x).lower()))\n",
    "    label_pipeline = lambda x: x\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text,_label) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list, label_list, offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fish(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(Fish, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # Fish encoder\n",
    "            nn.Linear(embed_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            # Fish decoder\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_class),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for layer in self.linear_relu_stack:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedding_output = self.embedding(text, offsets)\n",
    "        return self.softmax(self.linear_relu_stack(embedding_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 1000\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/ 5589 batches | accuracy    0.793\n",
      "| epoch   1 |  2000/ 5589 batches | accuracy    0.842\n",
      "| epoch   1 |  3000/ 5589 batches | accuracy    0.860\n",
      "| epoch   1 |  4000/ 5589 batches | accuracy    0.872\n",
      "| epoch   1 |  5000/ 5589 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 17.90s | valid accuracy    0.863 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |  1000/ 5589 batches | accuracy    0.880\n",
      "| epoch   2 |  2000/ 5589 batches | accuracy    0.885\n",
      "| epoch   2 |  3000/ 5589 batches | accuracy    0.892\n",
      "| epoch   2 |  4000/ 5589 batches | accuracy    0.894\n",
      "| epoch   2 |  5000/ 5589 batches | accuracy    0.900\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 16.86s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |  1000/ 5589 batches | accuracy    0.900\n",
      "| epoch   3 |  2000/ 5589 batches | accuracy    0.903\n",
      "| epoch   3 |  3000/ 5589 batches | accuracy    0.903\n",
      "| epoch   3 |  4000/ 5589 batches | accuracy    0.903\n",
      "| epoch   3 |  5000/ 5589 batches | accuracy    0.904\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 16.45s | valid accuracy    0.889 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |  1000/ 5589 batches | accuracy    0.908\n",
      "| epoch   4 |  2000/ 5589 batches | accuracy    0.912\n",
      "| epoch   4 |  3000/ 5589 batches | accuracy    0.906\n",
      "| epoch   4 |  4000/ 5589 batches | accuracy    0.916\n",
      "| epoch   4 |  5000/ 5589 batches | accuracy    0.918\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 16.86s | valid accuracy    0.873 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |  1000/ 5589 batches | accuracy    0.927\n",
      "| epoch   5 |  2000/ 5589 batches | accuracy    0.929\n",
      "| epoch   5 |  3000/ 5589 batches | accuracy    0.932\n",
      "| epoch   5 |  4000/ 5589 batches | accuracy    0.932\n",
      "| epoch   5 |  5000/ 5589 batches | accuracy    0.933\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 16.52s | valid accuracy    0.895 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |  1000/ 5589 batches | accuracy    0.936\n",
      "| epoch   6 |  2000/ 5589 batches | accuracy    0.933\n",
      "| epoch   6 |  3000/ 5589 batches | accuracy    0.935\n",
      "| epoch   6 |  4000/ 5589 batches | accuracy    0.935\n",
      "| epoch   6 |  5000/ 5589 batches | accuracy    0.936\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 16.50s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |  1000/ 5589 batches | accuracy    0.935\n",
      "| epoch   7 |  2000/ 5589 batches | accuracy    0.934\n",
      "| epoch   7 |  3000/ 5589 batches | accuracy    0.940\n",
      "| epoch   7 |  4000/ 5589 batches | accuracy    0.938\n",
      "| epoch   7 |  5000/ 5589 batches | accuracy    0.937\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 17.01s | valid accuracy    0.899 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |  1000/ 5589 batches | accuracy    0.942\n",
      "| epoch   8 |  2000/ 5589 batches | accuracy    0.939\n",
      "| epoch   8 |  3000/ 5589 batches | accuracy    0.938\n",
      "| epoch   8 |  4000/ 5589 batches | accuracy    0.939\n",
      "| epoch   8 |  5000/ 5589 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 16.49s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |  1000/ 5589 batches | accuracy    0.943\n",
      "| epoch   9 |  2000/ 5589 batches | accuracy    0.942\n",
      "| epoch   9 |  3000/ 5589 batches | accuracy    0.943\n",
      "| epoch   9 |  4000/ 5589 batches | accuracy    0.944\n",
      "| epoch   9 |  5000/ 5589 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 16.51s | valid accuracy    0.894 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |  1000/ 5589 batches | accuracy    0.944\n",
      "| epoch  10 |  2000/ 5589 batches | accuracy    0.943\n",
      "| epoch  10 |  3000/ 5589 batches | accuracy    0.945\n",
      "| epoch  10 |  4000/ 5589 batches | accuracy    0.942\n",
      "| epoch  10 |  5000/ 5589 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 16.51s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_class = 2\n",
    "vocab_size = len(vocab)\n",
    "emsize = 128\n",
    "model = Fish(vocab_size, emsize, num_class)\n",
    "\n",
    "train_sampler = train_dataset.get_sampler()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_batch, sampler=train_sampler)\n",
    "\n",
    "val_sampler = val_dataset.get_sampler()\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_batch, sampler=val_sampler)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "EPOCHS = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    model = train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, val_dataloader, criterion)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.925\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(model, test_dataloader, criterion)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(torch.tensor(pipeline(df_test.iloc[2][\"text\"])), torch.tensor([0])).argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = lambda x: vocab(tokenizer(str(x).lower()))\n",
    "predicted_labels = []\n",
    "for index, row in df_test.iterrows():\n",
    "    text = torch.tensor(pipeline(row[\"text\"]), dtype=torch.int64)\n",
    "    offsets = torch.tensor([0])\n",
    "    predicted_label = model(text, offsets)\n",
    "    predicted_labels.append(predicted_label.argmax(1).item())\n",
    "df_test[\"predicted_label\"] = predicted_labels\n",
    "df_test[\"error\"] = df_test[\"toxic\"] != df_test[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>introduction background an aortic ii signs sym...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>so i see now that calling you exactly as you s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>exactly what i was going to say ridiculous sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>to marissa behning and leanna feeney saying yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>google name origin goo ogle stare at the unusa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22329</th>\n",
       "      <td>possible racism of user rodhullandemu as this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22335</th>\n",
       "      <td>the article needs to say what he said on stage...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22344</th>\n",
       "      <td>it is sooooo gay</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22345</th>\n",
       "      <td>look user theresa knott hates</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22349</th>\n",
       "      <td>furthermore i fail to see why my statement of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1680 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic  \\\n",
       "3      introduction background an aortic ii signs sym...      0   \n",
       "12     so i see now that calling you exactly as you s...      0   \n",
       "17     exactly what i was going to say ridiculous sexism      0   \n",
       "21     to marissa behning and leanna feeney saying yo...      0   \n",
       "36     google name origin goo ogle stare at the unusa...      1   \n",
       "...                                                  ...    ...   \n",
       "22329  possible racism of user rodhullandemu as this ...      1   \n",
       "22335  the article needs to say what he said on stage...      0   \n",
       "22344                                   it is sooooo gay      0   \n",
       "22345                      look user theresa knott hates      0   \n",
       "22349  furthermore i fail to see why my statement of ...      0   \n",
       "\n",
       "       predicted_label  error  \n",
       "3                    1   True  \n",
       "12                   1   True  \n",
       "17                   1   True  \n",
       "21                   1   True  \n",
       "36                   0   True  \n",
       "...                ...    ...  \n",
       "22329                0   True  \n",
       "22335                1   True  \n",
       "22344                1   True  \n",
       "22345                1   True  \n",
       "22349                1   True  \n",
       "\n",
       "[1680 rows x 4 columns]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[\"error\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about sitush arrogant believes his arguments and references are best he ca nt digest logical still trying to get the biology definition in math book eg has written about khatris origin from dashrath sharma on rajputs book ca nt understand references and read them racist you come under this definition still if you have some shame left in you quit wikipedia or start listening discussing'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[32][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79d364cbb5ff81f54abd98f9f83d50f90d379b8661ccc369e0ce6b6f261137f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
